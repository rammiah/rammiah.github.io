<!DOCTYPE html>
<html lang="zh-cn">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>免费代理ip的爬取 | Hugo Site</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.71.0" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    
    
      <link href="/dist/css/app.1cb140d8ba31d5b2f1114537dd04802a.css" rel="stylesheet">
    

    

    
      
    

    
    
    <meta property="og:title" content="免费代理ip的爬取" />
<meta property="og:description" content="爬虫一直是python使用的一个重要部分，而许多网站也为此做了许多反爬措施，其中爬虫访问过于频繁直接封ip地址也作为一种“伤敌一千，自损八百”的方法被许多网站采用，代理ip便可以防止这种情况出现。
 搜索免费代理提供网站 提供免费代理的网站直接百度还是有很多的，这里我就选取了西刺免费代理，选取了透明代理进行爬取。
分析界面 打开网址，可以看见ip很规则的排列，可以说是对爬虫很友好了，Chrome打开F12分析这些代理的显示方式，可以知道是使用列表直接显示，还是比较简单的，单击下一页可以看见url里的最后多了个/2，页面的变换也可以很清楚的知道了。
在html源码中的分布，
进行爬取和测试有效性 分析完毕开始爬取ip，直接使用第三方的requests和BeautifulSoup4，可以让抓取变得很方便，代码如下：
from iptools import header, dict2proxy from bs4 import BeautifulSoup as Soup def parse_items(items): # 存放ip信息字典的列表 ips = [] for item in items: tds = item.find_all(&#39;td&#39;) # 从对应位置获取ip，端口，类型 ip, port, _type = tds[1].text, int(tds[2].text), tds[5].text ips.append({&#39;ip&#39;: ip, &#39;port&#39;: port, &#39;type&#39;: _type}) return ips def check_ip(ip): try: proxy = dict2proxy(ip) url = &#39;https://www.ipip.net/&#39; r = requests.get(url, headers=head, proxies=pro,timeout=5) r.raise_for_status() except: return False else: return True def get_proxies(index): url = &#39;http://www." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://rammiah.org/posts/%E5%85%8D%E8%B4%B9%E4%BB%A3%E7%90%86ip%E7%9A%84%E7%88%AC%E5%8F%96/" />
<meta property="article:published_time" content="2019-01-01T00:00:00+00:00" />
<meta property="article:modified_time" content="2019-01-01T00:00:00+00:00" />
<meta itemprop="name" content="免费代理ip的爬取">
<meta itemprop="description" content="爬虫一直是python使用的一个重要部分，而许多网站也为此做了许多反爬措施，其中爬虫访问过于频繁直接封ip地址也作为一种“伤敌一千，自损八百”的方法被许多网站采用，代理ip便可以防止这种情况出现。
 搜索免费代理提供网站 提供免费代理的网站直接百度还是有很多的，这里我就选取了西刺免费代理，选取了透明代理进行爬取。
分析界面 打开网址，可以看见ip很规则的排列，可以说是对爬虫很友好了，Chrome打开F12分析这些代理的显示方式，可以知道是使用列表直接显示，还是比较简单的，单击下一页可以看见url里的最后多了个/2，页面的变换也可以很清楚的知道了。
在html源码中的分布，
进行爬取和测试有效性 分析完毕开始爬取ip，直接使用第三方的requests和BeautifulSoup4，可以让抓取变得很方便，代码如下：
from iptools import header, dict2proxy from bs4 import BeautifulSoup as Soup def parse_items(items): # 存放ip信息字典的列表 ips = [] for item in items: tds = item.find_all(&#39;td&#39;) # 从对应位置获取ip，端口，类型 ip, port, _type = tds[1].text, int(tds[2].text), tds[5].text ips.append({&#39;ip&#39;: ip, &#39;port&#39;: port, &#39;type&#39;: _type}) return ips def check_ip(ip): try: proxy = dict2proxy(ip) url = &#39;https://www.ipip.net/&#39; r = requests.get(url, headers=head, proxies=pro,timeout=5) r.raise_for_status() except: return False else: return True def get_proxies(index): url = &#39;http://www.">
<meta itemprop="datePublished" content="2019-01-01T00:00:00&#43;00:00" />
<meta itemprop="dateModified" content="2019-01-01T00:00:00&#43;00:00" />
<meta itemprop="wordCount" content="422">



<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="免费代理ip的爬取"/>
<meta name="twitter:description" content="爬虫一直是python使用的一个重要部分，而许多网站也为此做了许多反爬措施，其中爬虫访问过于频繁直接封ip地址也作为一种“伤敌一千，自损八百”的方法被许多网站采用，代理ip便可以防止这种情况出现。
 搜索免费代理提供网站 提供免费代理的网站直接百度还是有很多的，这里我就选取了西刺免费代理，选取了透明代理进行爬取。
分析界面 打开网址，可以看见ip很规则的排列，可以说是对爬虫很友好了，Chrome打开F12分析这些代理的显示方式，可以知道是使用列表直接显示，还是比较简单的，单击下一页可以看见url里的最后多了个/2，页面的变换也可以很清楚的知道了。
在html源码中的分布，
进行爬取和测试有效性 分析完毕开始爬取ip，直接使用第三方的requests和BeautifulSoup4，可以让抓取变得很方便，代码如下：
from iptools import header, dict2proxy from bs4 import BeautifulSoup as Soup def parse_items(items): # 存放ip信息字典的列表 ips = [] for item in items: tds = item.find_all(&#39;td&#39;) # 从对应位置获取ip，端口，类型 ip, port, _type = tds[1].text, int(tds[2].text), tds[5].text ips.append({&#39;ip&#39;: ip, &#39;port&#39;: port, &#39;type&#39;: _type}) return ips def check_ip(ip): try: proxy = dict2proxy(ip) url = &#39;https://www.ipip.net/&#39; r = requests.get(url, headers=head, proxies=pro,timeout=5) r.raise_for_status() except: return False else: return True def get_proxies(index): url = &#39;http://www."/>

  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="http://rammiah.org/" class="f3 fw2 hover-white no-underline white-90 dib">
      Hugo Site
    </a>
    <div class="flex-l items-center">
      

      
      














    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  <article class="flex-l flex-wrap justify-between mw8 center ph3">
    <header class="mt4 w-100">
      <aside class="instapaper_ignoref b helvetica tracked">
          
        POSTS
      </aside>
      




  <div id="sharing" class="mt3">

    
    <a href="https://www.facebook.com/sharer.php?u=http://rammiah.org/posts/%E5%85%8D%E8%B4%B9%E4%BB%A3%E7%90%86ip%E7%9A%84%E7%88%AC%E5%8F%96/" class="facebook no-underline" aria-label="share on Facebook">
      <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M28.765,50.32h6.744V33.998h4.499l0.596-5.624h-5.095  l0.007-2.816c0-1.466,0.14-2.253,2.244-2.253h2.812V17.68h-4.5c-5.405,0-7.307,2.729-7.307,7.317v3.377h-3.369v5.625h3.369V50.32z   M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;"/></svg>

    </a>

    
    
    <a href="https://twitter.com/share?url=http://rammiah.org/posts/%E5%85%8D%E8%B4%B9%E4%BB%A3%E7%90%86ip%E7%9A%84%E7%88%AC%E5%8F%96/&amp;text=%e5%85%8d%e8%b4%b9%e4%bb%a3%e7%90%86ip%e7%9a%84%e7%88%ac%e5%8f%96" class="twitter no-underline" aria-label="share on Twitter">
      <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M37.167,22.283c-2.619,0.953-4.274,3.411-4.086,6.101  l0.063,1.038l-1.048-0.127c-3.813-0.487-7.145-2.139-9.974-4.915l-1.383-1.377l-0.356,1.017c-0.754,2.267-0.272,4.661,1.299,6.271  c0.838,0.89,0.649,1.017-0.796,0.487c-0.503-0.169-0.943-0.296-0.985-0.233c-0.146,0.149,0.356,2.076,0.754,2.839  c0.545,1.06,1.655,2.097,2.871,2.712l1.027,0.487l-1.215,0.021c-1.173,0-1.215,0.021-1.089,0.467  c0.419,1.377,2.074,2.839,3.918,3.475l1.299,0.444l-1.131,0.678c-1.676,0.976-3.646,1.526-5.616,1.568  C19.775,43.256,19,43.341,19,43.405c0,0.211,2.557,1.397,4.044,1.864c4.463,1.377,9.765,0.783,13.746-1.568  c2.829-1.673,5.657-5,6.978-8.221c0.713-1.716,1.425-4.851,1.425-6.354c0-0.975,0.063-1.102,1.236-2.267  c0.692-0.678,1.341-1.419,1.467-1.631c0.21-0.403,0.188-0.403-0.88-0.043c-1.781,0.636-2.033,0.551-1.152-0.402  c0.649-0.678,1.425-1.907,1.425-2.267c0-0.063-0.314,0.042-0.671,0.233c-0.377,0.212-1.215,0.53-1.844,0.72l-1.131,0.361l-1.027-0.7  c-0.566-0.381-1.361-0.805-1.781-0.932C39.766,21.902,38.131,21.944,37.167,22.283z M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4  s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/></svg>

    </a>

    
    <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http://rammiah.org/posts/%E5%85%8D%E8%B4%B9%E4%BB%A3%E7%90%86ip%E7%9A%84%E7%88%AC%E5%8F%96/&amp;title=%e5%85%8d%e8%b4%b9%e4%bb%a3%e7%90%86ip%e7%9a%84%e7%88%ac%e5%8f%96" class="linkedin no-underline" aria-label="share on LinkedIn">
      <svg  height="32px"  style="enable-background:new 0 0 65 65;" version="1.1" viewBox="0 0 65 65" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M50.837,48.137V36.425c0-6.275-3.35-9.195-7.816-9.195  c-3.604,0-5.219,1.983-6.119,3.374V27.71h-6.79c0.09,1.917,0,20.427,0,20.427h6.79V36.729c0-0.609,0.044-1.219,0.224-1.655  c0.49-1.22,1.607-2.483,3.482-2.483c2.458,0,3.44,1.873,3.44,4.618v10.929H50.837z M22.959,24.922c2.367,0,3.842-1.57,3.842-3.531  c-0.044-2.003-1.475-3.528-3.797-3.528s-3.841,1.524-3.841,3.528c0,1.961,1.474,3.531,3.753,3.531H22.959z M34,64  C17.432,64,4,50.568,4,34C4,17.431,17.432,4,34,4s30,13.431,30,30C64,50.568,50.568,64,34,64z M26.354,48.137V27.71h-6.789v20.427  H26.354z" style="fill-rule:evenodd;clip-rule:evenodd;fill:;"/>
</svg>

    </a>
  </div>


      <h1 class="f1 athelas mt3 mb1">免费代理ip的爬取</h1>
      
      
      <time class="f6 mv4 dib tracked" datetime="2019-01-01T00:00:00Z">January 1, 2019</time>

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l"><blockquote>
<p>爬虫一直是python使用的一个重要部分，而许多网站也为此做了许多反爬措施，其中爬虫访问过于频繁直接封ip地址也作为一种“伤敌一千，自损八百”的方法被许多网站采用，代理ip便可以防止这种情况出现。</p>
</blockquote>
<h2 id="搜索免费代理提供网站">搜索免费代理提供网站</h2>
<p>提供免费代理的网站直接百度还是有很多的，这里我就选取了<a href="http://www.xicidaili.com/">西刺免费代理</a>，选取了<a href="http://www.xicidaili.com/nt">透明代理</a>进行爬取。</p>
<h2 id="分析界面">分析界面</h2>
<p>打开网址，可以看见ip很规则的排列，可以说是对爬虫很友好了，Chrome打开F12分析这些代理的显示方式，可以知道是使用列表直接显示，还是比较简单的，单击下一页可以看见url里的最后多了个<code>/2</code>，页面的变换也可以很清楚的知道了。</p>
<p><img src="https://raw.githubusercontent.com/hustr/Pictures/master/xiciproxy/face.PNG" alt="界面"></p>
<p>在html源码中的分布，</p>
<p><img src="https://raw.githubusercontent.com/hustr/Pictures/master/xiciproxy/ip_show.png" alt="ip_show"></p>
<h2 id="进行爬取和测试有效性">进行爬取和测试有效性</h2>
<p>分析完毕开始爬取ip，直接使用第三方的requests和BeautifulSoup4，可以让抓取变得很方便，代码如下：</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> iptools <span style="color:#f92672">import</span> header, dict2proxy
<span style="color:#f92672">from</span> bs4 <span style="color:#f92672">import</span> BeautifulSoup <span style="color:#66d9ef">as</span> Soup

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">parse_items</span>(items):
    <span style="color:#75715e"># 存放ip信息字典的列表</span>
    ips <span style="color:#f92672">=</span> []
    <span style="color:#66d9ef">for</span> item <span style="color:#f92672">in</span> items:
        tds <span style="color:#f92672">=</span> item<span style="color:#f92672">.</span>find_all(<span style="color:#e6db74">&#39;td&#39;</span>)
        <span style="color:#75715e"># 从对应位置获取ip，端口，类型</span>
        ip, port, _type <span style="color:#f92672">=</span> tds[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>text, int(tds[<span style="color:#ae81ff">2</span>]<span style="color:#f92672">.</span>text), tds[<span style="color:#ae81ff">5</span>]<span style="color:#f92672">.</span>text
        ips<span style="color:#f92672">.</span>append({<span style="color:#e6db74">&#39;ip&#39;</span>: ip, <span style="color:#e6db74">&#39;port&#39;</span>: port, <span style="color:#e6db74">&#39;type&#39;</span>: _type})
    <span style="color:#66d9ef">return</span> ips

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">check_ip</span>(ip):
    <span style="color:#66d9ef">try</span>:
        proxy <span style="color:#f92672">=</span> dict2proxy(ip)
    	url <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;https://www.ipip.net/&#39;</span>
    	r <span style="color:#f92672">=</span> requests<span style="color:#f92672">.</span>get(url, headers<span style="color:#f92672">=</span>head, proxies<span style="color:#f92672">=</span>pro,timeout<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>)
        r<span style="color:#f92672">.</span>raise_for_status()
    <span style="color:#66d9ef">except</span>:
        <span style="color:#66d9ef">return</span> False
    <span style="color:#66d9ef">else</span>:
        <span style="color:#66d9ef">return</span> True
        
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_proxies</span>(index):
	url <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;http://www.xicidaili.com/nt/</span><span style="color:#e6db74">%d</span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">%</span> index
    r <span style="color:#f92672">=</span> requests<span style="color:#f92672">.</span>get(url, headers<span style="color:#f92672">=</span>header)
    r<span style="color:#f92672">.</span>encoding <span style="color:#f92672">=</span> r<span style="color:#f92672">.</span>apparent_encoding
    r<span style="color:#f92672">.</span>raise_for_status()
    soup <span style="color:#f92672">=</span> Soup(r<span style="color:#f92672">.</span>text, <span style="color:#e6db74">&#39;lxml&#39;</span>)
    <span style="color:#75715e"># 第一个是显示最上方的信息的，需要丢掉</span>
    items <span style="color:#f92672">=</span> soup<span style="color:#f92672">.</span>find_all(<span style="color:#e6db74">&#39;tr&#39;</span>)[<span style="color:#ae81ff">1</span>:]
    ips <span style="color:#f92672">=</span> parse_items(items)
    good_proxies <span style="color:#f92672">=</span> []
    <span style="color:#66d9ef">for</span> ip <span style="color:#f92672">in</span> ips:
        <span style="color:#66d9ef">if</span> check(ip):
            good_proxies<span style="color:#f92672">.</span>append(ip)
    <span style="color:#66d9ef">return</span> good_proxies
</code></pre></div><p>就像在上面写的，有效性我直接使用了<a href="https://www.ipip.net">ip查询网站</a>，获得的ip基本确保可以直接使用。</p>
<h2 id="写入json文件">写入json文件</h2>
<p>可以将获取的ip存放在json文件中，json模块的使用也很简单，直接打开一个文件，使用dump方法写入文件即可，</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> json

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">write_to_json</span>(ips):
    <span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#39;proxies.json&#39;</span>, <span style="color:#e6db74">&#39;w&#39;</span>, encoding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;utf-8&#39;</span>) <span style="color:#66d9ef">as</span> f:
        json<span style="color:#f92672">.</span>dump(ips, f, indent<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>)
</code></pre></div><p>写入后的文件内容如下：</p>
<p><img src="https://raw.githubusercontent.com/hustr/Pictures/master/xiciproxy/json.PNG" alt="json"></p>
<h2 id="写入mongodb">写入MongoDB</h2>
<p>写入数据库后获取和操作会很方便，</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> pymongo <span style="color:#f92672">import</span> MongoClient <span style="color:#66d9ef">as</span> Client

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">write_to_mongo</span>(ips):
    client <span style="color:#f92672">=</span> Client(host<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;localhost&#39;</span>, port<span style="color:#f92672">=</span><span style="color:#ae81ff">27017</span>)
    db <span style="color:#f92672">=</span> client[<span style="color:#e6db74">&#39;proxies_db&#39;</span>]
    coll <span style="color:#f92672">=</span> db[<span style="color:#e6db74">&#39;proxies&#39;</span>]
    <span style="color:#66d9ef">for</span> ip <span style="color:#f92672">in</span> ips:
        <span style="color:#66d9ef">if</span> coll<span style="color:#f92672">.</span>find({<span style="color:#e6db74">&#39;ip&#39;</span>: ip[<span style="color:#e6db74">&#39;ip&#39;</span>]})<span style="color:#f92672">.</span>count() <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
            coll<span style="color:#f92672">.</span>insert_one(ip)
    client<span style="color:#f92672">.</span>close()
</code></pre></div><p>写入后使用RoboMongo查看</p>
<p><img src="https://raw.githubusercontent.com/hustr/Pictures/master/xiciproxy/mongo.PNG" alt="mongo"></p>
<h2 id="使用多线程">使用多线程</h2>
<p>导入threading包，将Thread封装一下，得到最终的代码，</p>
<blockquote>
<p>get_proxies.py</p>
</blockquote>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> json

<span style="color:#f92672">import</span> requests
<span style="color:#f92672">import</span> time

<span style="color:#f92672">from</span> proxies_get.iptools <span style="color:#f92672">import</span> header, dict2proxy
<span style="color:#f92672">from</span> bs4 <span style="color:#f92672">import</span> BeautifulSoup <span style="color:#66d9ef">as</span> Soup
<span style="color:#f92672">from</span> pymongo <span style="color:#f92672">import</span> MongoClient <span style="color:#66d9ef">as</span> Client
<span style="color:#f92672">import</span> threading


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">parse_items</span>(items):
    <span style="color:#75715e"># 存放ip信息字典的列表</span>
    ips <span style="color:#f92672">=</span> []
    <span style="color:#66d9ef">for</span> item <span style="color:#f92672">in</span> items:
        tds <span style="color:#f92672">=</span> item<span style="color:#f92672">.</span>find_all(<span style="color:#e6db74">&#39;td&#39;</span>)
        <span style="color:#75715e"># 从对应位置获取ip，端口，类型</span>
        ip, port, _type <span style="color:#f92672">=</span> tds[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>text, int(tds[<span style="color:#ae81ff">2</span>]<span style="color:#f92672">.</span>text), tds[<span style="color:#ae81ff">5</span>]<span style="color:#f92672">.</span>text<span style="color:#f92672">.</span>lower()
        ips<span style="color:#f92672">.</span>append({<span style="color:#e6db74">&#39;ip&#39;</span>: ip, <span style="color:#e6db74">&#39;port&#39;</span>: port, <span style="color:#e6db74">&#39;type&#39;</span>: _type})

    <span style="color:#66d9ef">return</span> ips


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">check_ip</span>(ip, good_proxies):
    <span style="color:#66d9ef">try</span>:
        pro <span style="color:#f92672">=</span> dict2proxy(ip)
        <span style="color:#75715e"># print(pro)</span>
        url <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;https://www.ipip.net/&#39;</span>
        r <span style="color:#f92672">=</span> requests<span style="color:#f92672">.</span>get(url, headers<span style="color:#f92672">=</span>header, proxies<span style="color:#f92672">=</span>pro, timeout<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>)
        r<span style="color:#f92672">.</span>raise_for_status()
        <span style="color:#66d9ef">print</span>(r<span style="color:#f92672">.</span>status_code, ip[<span style="color:#e6db74">&#39;ip&#39;</span>])
    <span style="color:#66d9ef">except</span> <span style="color:#a6e22e">Exception</span> <span style="color:#66d9ef">as</span> e:
        <span style="color:#75715e"># print(e)</span>
        <span style="color:#66d9ef">pass</span>
    <span style="color:#66d9ef">else</span>:
        good_proxies<span style="color:#f92672">.</span>append(ip)


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">write_to_json</span>(ips):
    <span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#39;proxies.json&#39;</span>, <span style="color:#e6db74">&#39;w&#39;</span>, encoding<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;utf-8&#39;</span>) <span style="color:#66d9ef">as</span> f:
        json<span style="color:#f92672">.</span>dump(ips, f, indent<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>)


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">write_to_mongo</span>(ips):
    <span style="color:#e6db74">&#39;&#39;&#39;将数据写入mongoDB&#39;&#39;&#39;</span>
    client <span style="color:#f92672">=</span> Client(host<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;localhost&#39;</span>, port<span style="color:#f92672">=</span><span style="color:#ae81ff">27017</span>)
    db <span style="color:#f92672">=</span> client[<span style="color:#e6db74">&#39;proxies_db&#39;</span>]
    coll <span style="color:#f92672">=</span> db[<span style="color:#e6db74">&#39;proxies&#39;</span>]
    <span style="color:#75715e"># 先检测，再写入，防止重复</span>
    <span style="color:#66d9ef">for</span> ip <span style="color:#f92672">in</span> ips:
        <span style="color:#66d9ef">if</span> coll<span style="color:#f92672">.</span>find({<span style="color:#e6db74">&#39;ip&#39;</span>: ip[<span style="color:#e6db74">&#39;ip&#39;</span>]})<span style="color:#f92672">.</span>count() <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
            coll<span style="color:#f92672">.</span>insert_one(ip)
    client<span style="color:#f92672">.</span>close()


<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">GetThread</span>(threading<span style="color:#f92672">.</span>Thread):
    <span style="color:#e6db74">&#39;&#39;&#39;对Thread进行封装&#39;&#39;&#39;</span>
    <span style="color:#66d9ef">def</span> __init__(self, args):
        threading<span style="color:#f92672">.</span>Thread<span style="color:#f92672">.</span>__init__(self, args<span style="color:#f92672">=</span>args)
        self<span style="color:#f92672">.</span>good_proxies <span style="color:#f92672">=</span> []

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">run</span>(self):
        url <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;http://www.xicidaili.com/nt/</span><span style="color:#e6db74">%d</span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">%</span> self<span style="color:#f92672">.</span>_args[<span style="color:#ae81ff">0</span>]
        <span style="color:#75715e"># 发起网络访问</span>
        r <span style="color:#f92672">=</span> requests<span style="color:#f92672">.</span>get(url, headers<span style="color:#f92672">=</span>header)
        r<span style="color:#f92672">.</span>encoding <span style="color:#f92672">=</span> r<span style="color:#f92672">.</span>apparent_encoding
        r<span style="color:#f92672">.</span>raise_for_status()
        soup <span style="color:#f92672">=</span> Soup(r<span style="color:#f92672">.</span>text, <span style="color:#e6db74">&#39;lxml&#39;</span>)
        <span style="color:#75715e"># 第一个是显示最上方的信息的，需要丢掉</span>
        items <span style="color:#f92672">=</span> soup<span style="color:#f92672">.</span>find_all(<span style="color:#e6db74">&#39;tr&#39;</span>)[<span style="color:#ae81ff">1</span>:]
        ips <span style="color:#f92672">=</span> parse_items(items)
        threads <span style="color:#f92672">=</span> []
        <span style="color:#66d9ef">for</span> ip <span style="color:#f92672">in</span> ips:
            <span style="color:#75715e"># 开启多线程</span>
            t <span style="color:#f92672">=</span> threading<span style="color:#f92672">.</span>Thread(target<span style="color:#f92672">=</span>check_ip, args<span style="color:#f92672">=</span>[ip, self<span style="color:#f92672">.</span>good_proxies])
            t<span style="color:#f92672">.</span>start()
            time<span style="color:#f92672">.</span>sleep(<span style="color:#ae81ff">0.1</span>)
            threads<span style="color:#f92672">.</span>append(t)
        [t<span style="color:#f92672">.</span>join() <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> threads]

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_result</span>(self):
        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>good_proxies


<span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;__main__&#39;</span>:
    <span style="color:#75715e"># 主函数使用多线程</span>
    threads <span style="color:#f92672">=</span> []
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">30</span>):
        t <span style="color:#f92672">=</span> GetThread(args<span style="color:#f92672">=</span>[i])
        t<span style="color:#f92672">.</span>start()
        time<span style="color:#f92672">.</span>sleep(<span style="color:#ae81ff">10</span>)
        threads<span style="color:#f92672">.</span>append(t)
    [t<span style="color:#f92672">.</span>join() <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> threads]
    <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> threads:
        proxies <span style="color:#f92672">=</span> t<span style="color:#f92672">.</span>get_result()
        write_to_mongo(proxies)

</code></pre></div><blockquote>
<p>iptools.py</p>
</blockquote>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">header <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#39;User-Agent&#39;</span>: <span style="color:#e6db74">&#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) &#39;</span>
                        <span style="color:#e6db74">&#39;AppleWebKit/537.36 (KHTML, like Gecko) &#39;</span>
                        <span style="color:#e6db74">&#39;Chrome/64.0.3282.186 Safari/537.36&#39;</span>}


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">dict2proxy</span>(dic):
    s <span style="color:#f92672">=</span> dic[<span style="color:#e6db74">&#39;type&#39;</span>] <span style="color:#f92672">+</span> <span style="color:#e6db74">&#39;://&#39;</span> <span style="color:#f92672">+</span> dic[<span style="color:#e6db74">&#39;ip&#39;</span>] <span style="color:#f92672">+</span> <span style="color:#e6db74">&#39;:&#39;</span> <span style="color:#f92672">+</span> str(dic[<span style="color:#e6db74">&#39;port&#39;</span>])
    <span style="color:#66d9ef">return</span> {<span style="color:#e6db74">&#39;http&#39;</span>: s, <span style="color:#e6db74">&#39;https&#39;</span>: s}

</code></pre></div><h2 id="总结">总结</h2>
<p>这个免费代理ip的爬虫没什么太难的地方，就是服务器有点弱，一不小心就503了，需要限制一下访问速度。</p>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="http://rammiah.org/" >
    &copy;  Hugo Site 2020 
  </a>
    <div>













</div>
  </div>
</footer>

    

  <script src="/dist/js/app.3fc0f988d21662902933.js"></script>


  </body>
</html>
